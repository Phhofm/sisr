
name = "2xBHI_aether_medium_l1pretrain_quant"
model_type = "image"
scale = 2
use_amp = false # Please use fp32, especially with qat. Best speed, and quality.
bfloat16 = false
fast_matmul = true

[datasets.train]
type = "paired"
dataroot_gt = '/home/phips/Documents/dataset/PDM/OSISRD/v3/hr'
dataroot_lq = '/home/phips/Documents/dataset/PDM/OSISRD/v3/x2'
patch_size = 64
batch_size = 8

[datasets.val]
name = "val"
type = "paired"
dataroot_gt = '/home/phips/Documents/dataset/PDM/OSISRD/v3/validation/hr'
dataroot_lq = '/home/phips/Documents/dataset/PDM/OSISRD/v3/validation/x2'
[val]
val_freq = 1000
[val.metrics.psnr]
type = "calculate_psnr"
[val.metrics.ssim]
type = "calculate_ssim"
[val.metrics.dists]
type = "calculate_dists"
better = "lower"
[val.metrics.topiq]
type = "calculate_topiq"

[path]

[network_g]
#type = "aether_small"
type = "aether_medium"
#type = "aether_large"

[train]
ema = 0.999
enable_qat = true # Only disable qat training if you will never!!! create an int8 onnx model out of this trained model. QAT adds a bit of training overhead and also worsen metrics slightly, but will be profitable for the end user.

[train.optim_g]
type = "adamw"
lr = 2e-4
betas = [ 0.9, 0.99 ]
weight_decay = 0

#  losses
[train.pixel_opt]
type = "L1Loss"
loss_weight = 1.0
reduction = "mean"

[logger]
total_iter = 1000000
save_checkpoint_freq = 1000
use_tb_logger = true
